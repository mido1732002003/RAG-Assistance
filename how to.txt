Alternative: Manual Setup
Bash

# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Copy environment file
cp .env.example .env

# Download models
python scripts/download_models.py

# Initialize database
python scripts/migrate_db.py

# Run the server
uvicorn app.main:app --reload
Docker
Bash

# Build and run with Docker Compose
make docker-up

# Or manually
docker-compose up -d
Configuration
Edit .env file:

env

# Folders to watch (comma-separated)
WATCH_DIRS=./data,/path/to/documents

# Offline mode (no external API calls)
OFFLINE_MODE=true

# Optional: LLM API keys
OPENAI_API_KEY=sk-...
MISTRAL_API_KEY=...
Usage
Web Interface
Open http://localhost:8000/chat in your browser.

API
Interactive docs at http://localhost:8000/docs

Python

import requests

# Chat endpoint
response = requests.post("http://localhost:8000/api/chat/", json={
    "query": "What is the main topic of the documents?",
    "top_k": 5
})

# Search endpoint
response = requests.get("http://localhost:8000/api/search/", params={
    "q": "machine learning",
    "top_k": 10
})
CLI
Bash

# Ingest documents
python cli/main.py ingest add /path/to/documents

# Search
python cli/main.py search query "your question"

# Backup
python cli/main.py admin backup

# Restore
python cli/main.py admin restore backup_20240101_120000.tar.gz
Folder Structure
text

local-rag-assistant/
├── data/           # Default watch directory
├── var/
│   ├── index/      # FAISS index
│   ├── rag.db      # SQLite database
│   └── logs/       # Application logs
└── snapshots/      # Backup snapshots
Troubleshooting
Windows Issues
Long file paths: Enable long path support in Windows
Permission errors: Run as administrator or check folder permissions
Performance
Adjust BATCH_SIZE and MAX_WORKERS in .env
For large documents, increase CHUNK_SIZE
Use GPU if available (automatically detected)
Memory Issues
Reduce BATCH_SIZE for embedding generation
Use smaller embedding models
Enable incremental indexing
Development
Bash

# Run tests
make test

# Lint code
make lint

# Format code
make format
License
MIT License - See LICENSE file for details.

text


### ARCHITECTURE.md
```markdown
# Architecture Overview

## System Design
┌─────────────────────────────────────────────────────────────┐
│ User Interface │
│ (Web UI / CLI / API) │
└─────────────────────────┬───────────────────────────────────┘
│
┌─────────────────────────▼───────────────────────────────────┐
│ FastAPI Application │
│ ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐ │
│ │ Chat │ │ Search │ │ Ingest │ │ Admin │ │
│ │Endpoints │ │Endpoints │ │Endpoints │ │Endpoints │ │
│ └──────────┘ └──────────┘ └──────────┘ └──────────┘ │
└─────────────────────────┬───────────────────────────────────┘
│
┌─────────────────────────▼───────────────────────────────────┐
│ Core Components │
│ ┌──────────────┐ ┌──────────────┐ ┌──────────────┐ │
│ │ Retriever │ │ Generator │ │ Ingestion │ │
│ │ │ │ │ │ Pipeline │ │
│ │ - Vector │ │ - Offline │ │ │ │
│ │ - BM25 │ │ - LLM │ │ - Parser │ │
│ │ - Reranker │ │ │ │ - Chunker │ │
│ └──────────────┘ └──────────────┘ └──────────────┘ │
└─────────────────────────┬───────────────────────────────────┘
│
┌─────────────────────────▼───────────────────────────────────┐
│ Storage Layer │
│ ┌──────────────┐ ┌──────────────┐ ┌──────────────┐ │
│ │ Vector Store │ │Document Store│ │ Cache │ │
│ │ (FAISS) │ │ (SQLite) │ │ (File) │ │
│ └──────────────┘ └──────────────┘ └──────────────┘ │
└───────────────────────────────────────────────────────────────┘

## Data Flow

### Ingestion Flow

1. **File Detection**: Watcher detects new/modified files
2. **Parsing**: Extract text from various formats
3. **Preprocessing**: Clean and normalize text
4. **Language Detection**: Identify document language
5. **Chunking**: Split into semantic chunks with overlap
6. **Deduplication**: Remove duplicate chunks
7. **Embedding**: Generate vector embeddings
8. **Storage**: Save to FAISS index and SQLite

### Retrieval Flow

1. **Query Processing**: Clean and prepare query
2. **Hybrid Search**:
   - Vector similarity search (FAISS)
   - Keyword search (BM25)
   - Score fusion
3. **Reranking**: Optional cross-encoder reranking
4. **Result Compilation**: Format with metadata

### Generation Flow

1. **Context Preparation**: Format retrieved chunks
2. **Mode Selection**:
   - Offline: Extractive summarization
   - LLM: API call with context
3. **Citation Extraction**: Link answer to sources
4. **Response Formatting**: Structure final response

## Database Schema

### Documents Table
- id (PRIMARY KEY)
- file_path (UNIQUE)
- filename
- file_size
- mime_type
- checksum
- language
- title
- created_at
- modified_at
- indexed_at

### Chunks Table
- id (PRIMARY KEY)
- document_id (FOREIGN KEY)
- chunk_index
- text
- start_char
- end_char
- page_number
- metadata_json
- embedding_id

### Queries Table
- id (PRIMARY KEY)
- query_text
- response_text
- mode
- provider
- top_k
- timestamp
- processing_time
- user_rating

### Citations Table
- id (PRIMARY KEY)
- query_id (FOREIGN KEY)
- chunk_id (FOREIGN KEY)
- relevance_score
- position

## Consistency Model

- **Eventually Consistent**: File changes are processed asynchronously
- **Checksum-based**: Detect actual content changes
- **Transactional**: Database operations are atomic
- **Incremental**: Only changed files are reprocessed

## Performance Considerations

- **Batch Processing**: Embeddings generated in batches
- **Caching**: Query results cached with TTL
- **Lazy Loading**: Models loaded on first use
- **Index Persistence**: FAISS index saved to disk
- **Connection Pooling**: Reuse database connections
Custom Dataset
Create a JSON file:

JSON

{
  "queries": [
    {
      "id": "q1",
      "question": "What is machine learning?",
      "relevant_docs": ["doc1.pdf", "doc2.pdf"],
      "expected_answer": "Machine learning is..."
    }
  ]
}
Run evaluation:

Bash

python eval/runner.py --dataset custom_qa.json --metrics all
Metrics Output
text

Evaluation Results:
─────────────────────────────
Retrieval Metrics:
  Precision@5: 0.82
  Recall@5: 0.65
  MRR: 0.73
  NDCG@5: 0.78

Generation Metrics:
  BLEU-4: 0.41
  ROUGE-L: 0.56
  BERTScore: 0.87
A/B Testing
Configuration
Python

# eval/ab_test.py
configs = {
    "baseline": {
        "chunk_size": 512,
        "top_k": 5,
        "mode": "offline"
    },
    "variant": {
        "chunk_size": 256,
        "top_k": 10,
        "mode": "llm"
    }
}
Running Tests
Bash

python eval/ab_test.py --configs baseline,variant --dataset sample_qa.json
Continuous Evaluation
Automated Testing
Add to CI/CD pipeline:

YAML

# .github/workflows/eval.yml
- name: Run Evaluation
  run: |
    python eval/runner.py --dataset eval/datasets/ci_test.json
    python eval/check_regression.py --threshold 0.7
Performance Tracking
Track metrics over time:

Bash

python eval/track_performance.py --output metrics_history.csv
Human Evaluation
Rating Collection
Enable in UI:

Python

# app/api/chat.py
@router.post("/rate")
async def rate_response(
    query_id: int,
    rating: int  # 1-5 stars
):
    # Save rating to database
Analysis
Bash

python eval/analyze_ratings.py --min-ratings 10
Troubleshooting
Low Precision
Increase reranking threshold
Improve chunking strategy
Add more stop words
Low Recall
Increase top_k parameter
Reduce chunk overlap
Improve query expansion
Poor Generation
Increase context window
Improve prompt engineering
Fine-tune reranker
text


This completes the full implementation of the Local RAG Assistant. The system is now ready to:

1. **Run locally** with `make setup && make dev`
2. **Index documents** from configured directories
3. **Answer questions** using retrieved context
4. **Work offline** or with optional LLM APIs
5. **Provide citations** for all answers

The implementation includes all requested features with clean, type-hinted code, comprehensive error handling, and cross-platform compatibility.